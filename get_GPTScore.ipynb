{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9745ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_prompt_open = \"\"\"\n",
    "I need your help to evaluate the performance of several models in the speech interaction scenario. The models will receive a speech input from the user, which they need to understand and respond to with a speech output.\n",
    "Your task is to rate the model’s responses based on the provided user input transcription [Instruction] and the model’s output transcription [Response].\n",
    "\n",
    "Please evaluate the response on a scale of 1 to 5:\n",
    "1 point: The response is largely irrelevant, incorrect, or fails to address the user’s query. It may be off-topic or provide incorrect information.\n",
    "2 points: The response is somewhat relevant but lacks accuracy or completeness. It may only partially answer the user’s question or include extraneous information.\n",
    "3 points: The response is relevant and mostly accurate, but it may lack conciseness or include unnecessary details that don’t contribute to the main point.\n",
    "4 points: The response is relevant, accurate, and concise, providing a clear answer to the user’s question without unnecessary elaboration.\n",
    "5 points: The response is exceptionally relevant, accurate, and to the point. It directly addresses the user’s query in a highly effective and efficient manner, providing exactly the information needed.\n",
    "\n",
    "Below are the transcription of user’s instruction and models’ response:\n",
    "### [Instruction]: {prompt}\n",
    "### [Response]: {response}\n",
    "\n",
    "After evaluating, please output the score only without anything else.\n",
    "You don’t need to provide any explanations.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_prompt_ref = \"\"\"\n",
    "I need your help to evaluate the performance of several models in the speech interaction scenario. The models will receive a speech input from the user, which they need to understand and respond to with a speech output.\n",
    "Your task is to rate the model’s responses based on the provided user input transcription [Instruction], the model’s output transcription [Response], and the basic reference answer [Reference].\n",
    "\n",
    "Please evaluate the response on a scale of 1 to 5:\n",
    "1 point: The response is largely irrelevant, incorrect, or fails to address the user’s query. It may be off-topic or provide incorrect information.\n",
    "2 points: The response is somewhat relevant but lacks accuracy, completeness, or alignment with the user’s query. It may only partially answer the question or include extraneous content.\n",
    "3 points: The response is generally accurate and relevant, providing information consistent with the reference answer but may lack depth, clarity, or contextual adaptation.\n",
    "4 points: The response is accurate, relevant, and contextually appropriate, not only matching the reference but also providing a clear and well-structured answer that aligns well with the user’s query.\n",
    "5 points: The response is exceptionally accurate, relevant, and informative. It goes beyond the basic reference answer, offering richer, clearer, or more contextually appropriate information while fully addressing the user’s query.\n",
    "\n",
    "Below are the transcription of user’s instruction, reference answer, and model’s response:\n",
    "### [Instruction]: {prompt}\n",
    "### [Reference]: {reference}\n",
    "### [Response]: {response}\n",
    "\n",
    "After evaluating, please output the score only without anything else.\n",
    "You don’t need to provide any explanations.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e383e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(context_text):\n",
    "    return context_text.replace(\"～\",\"\").replace(\"⁇\",\"\").replace(\"✎\",\"\").replace(\"↻\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "\n",
    "def getConn(message):\n",
    "    # 创建连接和准备请求\n",
    "    conn = http.client.HTTPSConnection(\"api.chatfire.cn\")\n",
    "    payload = json.dumps({\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": message,\n",
    "        \"temperature\": 0\n",
    "    }, ensure_ascii=False).encode('utf-8')\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json; charset=utf-8',\n",
    "        'Authorization': ''  # Your API key here\n",
    "    }\n",
    "    \n",
    "    # 发送请求\n",
    "    conn.request(\"POST\", \"/v1/chat/completions\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    \n",
    "    # 检查HTTP状态码\n",
    "    if res.status != 200:\n",
    "        print(json.loads(res.read().decode(\"utf-8\")))\n",
    "        print(f\"API返回错误状态码：{res.status}\")\n",
    "    \n",
    "    data = res.read().decode(\"utf-8\")\n",
    "    json_data = json.loads(data)\n",
    "    \n",
    "    content = json_data['choices'][0]['message']['content']\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "global task\n",
    "global is_reference\n",
    "def chat_completions3(query):\n",
    "    resp = getConn(query)\n",
    "    return resp\n",
    "\n",
    "def generate(item):\n",
    "    if task == 'multimodality_chat':\n",
    "        question = item['混合模态对话']['prompt']\n",
    "        response = item['混合模态对话']['预测回复']\n",
    "    else:\n",
    "        question = item['对话']['prompt']\n",
    "        response = item['对话']['预测回复']\n",
    "    \n",
    "    if not is_reference:\n",
    "        prompt = meta_prompt_open.replace(\"{prompt}\", question).replace('{response}', response)\n",
    "    else:\n",
    "        if item['next'] != {}:\n",
    "            reference = clean_text(item['next'][\"text\"])\n",
    "            prompt = meta_prompt_ref.replace(\"{prompt}\", question).replace('{response}', response).replace('{reference}', reference)\n",
    "        else:\n",
    "            prompt = meta_prompt_open.replace(\"{prompt}\", question).replace('{response}', response)\n",
    "        \n",
    "    for _ in range(25):\n",
    "        try:\n",
    "            score = chat_completions3(\n",
    "                    query=[{\"role\": \"system\",\n",
    "                            \"content\": \"You are a helpful assistant who tries to help answer the user's question.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"发生错误: {e}\")\n",
    "            score = \"0\"\n",
    "            time.sleep(5)\n",
    "    time.sleep(10)\n",
    "    if not is_reference:\n",
    "        item['score'] = score\n",
    "    else:\n",
    "        item['score_ref'] = score\n",
    "        item['total_score'] = str(int(item['score']) + int(score) )\n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859cbd99",
   "metadata": {},
   "source": [
    "# Evaluation Without Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ced78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_reference = False\n",
    "\n",
    "for model_dir in os.listdir('results'): \n",
    "    model_path = os.path.join('results', model_dir)\n",
    "    if not os.path.isdir(model_path) or model_dir.startswith('.'):\n",
    "        continue\n",
    "    \n",
    "    for subset_dir in os.listdir(model_path):\n",
    "        subset_path = os.path.join(model_path, subset_dir)\n",
    "        if not os.path.isdir(subset_path):\n",
    "            continue\n",
    "        \n",
    "        for json_path in glob.glob(os.path.join(subset_path, '*-fix.json')):\n",
    "            model = json_path.split('/')[-3]\n",
    "            task = json_path.split('/')[-2]\n",
    "            subset = json_path.split('/')[-1].split('-')[0]\n",
    "            scores = []\n",
    "            if task == 'multimodality_chat' or task == 'chat':\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    pre_items = json.load(f)\n",
    "                    with multiprocessing.Pool(128) as pool:\n",
    "                        results = list(tqdm(pool.imap(generate, pre_items), total=len(pre_items)))\n",
    "\n",
    "                    for score in results:\n",
    "                        if 'score' in score:\n",
    "                            scores.append(int(score['score']))\n",
    "                        else:\n",
    "                            scores.append(0)\n",
    "                avg_score = sum(scores) / len(scores) if scores else 0\n",
    "                results.insert(0, {\"avg_score\": avg_score})\n",
    "                \n",
    "                tgt_file = json_path.replace('-fix.json', '-fix_metrics.json')\n",
    "                with open(tgt_file, \"w\") as file:\n",
    "                    json.dump(results, file, ensure_ascii=False, indent=4)\n",
    "                print(f'avg_score: {avg_score}')\n",
    "                print(f\"Processed {json_path} and saved to {tgt_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b888a",
   "metadata": {},
   "source": [
    "# Evaluation With Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a274771",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_reference = True\n",
    "\n",
    "for model_dir in os.listdir('results'): \n",
    "    model_path = os.path.join('results', model_dir)\n",
    "    if not os.path.isdir(model_path) or model_dir.startswith('.'):\n",
    "        continue\n",
    "    \n",
    "    for subset_dir in os.listdir(model_path):\n",
    "        subset_path = os.path.join(model_path, subset_dir)\n",
    "        if not os.path.isdir(subset_path):\n",
    "            continue\n",
    "        \n",
    "\n",
    "        for json_path in glob.glob(os.path.join(subset_path, '*-fix_metrics.json')):\n",
    "            model = json_path.split('/')[-3]\n",
    "            task = json_path.split('/')[-2]\n",
    "            subset = json_path.split('/')[-1].split('-')[0]\n",
    "            scores = []\n",
    "            if task == 'multimodality_chat' or task == 'chat':\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    all_data = json.load(f)\n",
    "                    avg = all_data[0]\n",
    "                    pre_items = all_data[1:]\n",
    "                    \n",
    "                    with multiprocessing.Pool(256) as pool:\n",
    "                        results = list(tqdm(pool.imap(generate, pre_items), total=len(pre_items)))\n",
    "\n",
    "                    for score in results:\n",
    "                        if 'score_ref' in score:\n",
    "                            scores.append(int(score['score_ref']))\n",
    "                        else:\n",
    "                            scores.append(0)\n",
    "\n",
    "                avg_score = sum(scores) / len(scores) if scores else 0\n",
    "                avg[\"avg_score_ref\"] =  avg_score\n",
    "                avg[\"total_score\"] =  avg_score + avg[\"avg_score\"]\n",
    "                \n",
    "                print(f'Avg_score {avg[\"avg_score\"]}')\n",
    "                print(f'Avg_score_ref {avg_score}')\n",
    "                print(f'total_score {avg[\"total_score\"]}')\n",
    "                results.insert(0, avg)\n",
    "                tgt_file = json_path\n",
    "                with open(tgt_file, \"w\") as file:\n",
    "                    json.dump(results, file, ensure_ascii=False, indent=4)\n",
    "                print(f\"Processed {json_path} and saved to {tgt_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
